{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering \n",
    "\n",
    "이 가이드에서는 GPT-4o와 같은 대규모 언어 모델(GPT 모델이라고도 함)에서 더 나은 결과를 얻기 위한 전략과 전술을 공유합니다. 여기에 설명된 방법들은 때때로 더 큰 효과를 위해 조합하여 배포할 수 있습니다. 자신에게 가장 적합한 방법을 찾기 위해 실험해 보시기 바랍니다. 또한 모델 기능을 보여주는 예시 프롬프트도 살펴볼 수 있습니다:\n",
    "\n",
    "## 더 나은 결과를 얻기 위한 6가지 전략\n",
    "\n",
    "### 1. 명확한 지침 작성\n",
    "대형언어모델은 사용자의 마음을 읽을 수 없습니다. 답변이 너무 길면 간단한 답변을 요청하세요. 답변이 너무 간단하다면 전문가 수준의 문장을 요청하세요. 형식이 마음에 들지 않으면 원하는 형식을 직접 보여주세요. 모델이 원하는 것을 추측할 필요가 적을수록 원하는 것을 얻을 가능성이 높아집니다.\n",
    "\n",
    "#### 전술:\n",
    "\n",
    "1. 쿼리에 세부 정보를 포함하여 보다 관련성 높은 답변 얻기\n",
    "2. 모델에 페르소나를 채택하도록 요청하기 \n",
    "3. 구분 기호를 사용하여 입력의 구분되는 부분을 명확하게 표시하기 \n",
    "4. 작업 완료에 필요한 단계 지정하기 \n",
    "5. 예제 제공 \n",
    "6. 출력의 원하는 길이 지정하기\n",
    "\n",
    "### 2. 참조 텍스트 제공\n",
    "언어 모델은 특히 난해한 주제나 인용 및 URL에 대한 질문을 받을 때 자신 있게 가짜 답변을 만들어낼 수 있습니다. 노트 한 장이 시험 성적을 높이는 데 도움이 되는 것과 마찬가지로, 이러한 모델에 참조 텍스트를 제공하면 더 적은 조작으로 답안을 작성하는 데 도움이 될 수 있습니다.\n",
    "\n",
    "#### 전술:\n",
    "\n",
    "1. 모델에게 참조 텍스트를 사용하여 답하도록 지시하기 \n",
    "2. 모델에게 참조 텍스트의 인용을 사용하여 답하도록 지시하기\n",
    "\n",
    "### 3. 복잡한 작업을 더 간단한 하위 작업으로 나누기 \n",
    "소프트웨어 엔지니어링에서 복잡한 시스템을 일련의 모듈식 구성 요소로 분해하는 것이 좋은 관행인 것처럼 언어 모델에 제출된 작업도 마찬가지입니다. 복잡한 작업은 단순한 작업보다 오류율이 높은 경향이 있습니다. 또한 복잡한 작업은 종종 이전 작업의 출력을 사용하여 이후 작업의 입력을 구성하는 더 간단한 작업의 워크플로로 재정의할 수 있습니다.\n",
    "\n",
    "#### 전술:\n",
    "\n",
    "1. 의도 분류를 사용하여 사용자 쿼리에 가장 관련성이 높은 지침 식별 \n",
    "2. 매우 긴 대화가 필요한 대화 애플리케이션의 경우 이전 대화를 요약하거나 필터링 \n",
    "3. 긴 문서를 부분적으로 요약하고 전체 요약을 재귀적으로 구성\n",
    "\n",
    "### 4.모델에 '생각할 시간' 주기 \n",
    "\n",
    "17에 28을 곱하라는 질문을 받으면 즉시 알 수는 없지만 시간이 지나면 풀 수 있습니다. 마찬가지로 모델은 시간을 들여 답을 찾는 것보다 바로 답을 찾으려고 할 때 더 많은 추론 오류를 범합니다. 답을 내기 전에 '생각의 연쇄(Chain of Thought)'를 요청하면 모델이 보다 안정적으로 정답을 추론하는 데 도움이 될 수 있습니다.\n",
    "\n",
    "#### 전술:\n",
    "\n",
    "1. 결론을 내리기 전에 모델 스스로 해결책을 찾도록 지시하기 \n",
    "2. 모델의 추론 과정을 숨기기 위해 내부 독백 또는 일련의 쿼리를 사용하기 \n",
    "3. 모델에게 이전 통과에서 놓친 것이 있는지 물어보기\n",
    "\n",
    "### 5. 외부 도구 사용 \n",
    "다른 도구의 출력을 모델에 공급하여 모델의 약점을 보완하세요. 예를 들어, 텍스트 검색 시스템(RAG 또는 검색 증강 생성이라고도 함)은 모델에 관련 문서에 대해 알려줄 수 있습니다. 코드 인터프리터와 같은 코드 실행 엔진은 모델이 수학을 수행하고 코드를 실행하는 데 도움을 줄 수 있습니다. 언어 모델보다 도구로 더 안정적이고 효율적으로 작업을 수행할 수 있다면, 그 작업을 오프로드하여 두 가지 장점을 모두 활용하세요.\n",
    "\n",
    "#### 전술:\n",
    "\n",
    "1. 임베딩 기반 검색을 사용하여 효율적인 지식 검색 구현 \n",
    "2. 코드 실행을 사용하여 보다 정확한 계산을 수행하거나 외부 API 호출 \n",
    "3. 모델에 특정 기능에 대한 액세스 권한 부여\n",
    "\n",
    "### 6. 체계적으로 변경 사항 테스트하기 \n",
    "성능을 측정할 수 있다면 성능 개선이 더 쉬워집니다. 프롬프트를 수정하면 일부 고립된 예제에서는 성능이 향상되지만 보다 대표적인 예제 집합에서는 전반적인 성능이 저하되는 경우가 있습니다. 따라서 변경 사항이 성능에 긍정적인 영향을 미치는지 확인하려면 포괄적인 테스트 모음('평가'라고도 함)을 정의해야 할 수도 있습니다.\n",
    "\n",
    "#### 전술:\n",
    "\n",
    "1. 표준 답변을 참조하여 모델 결과 평가하기\n",
    "\n",
    "## 전술 \n",
    "위에 나열된 각 전략은 구체적인 전술로 구현할 수 있습니다. 이러한 전술은 시도해 볼 수 있는 아이디어를 제공하기 위한 것입니다. 이 전략이 완전히 포괄적인 것은 아니므로 여기에 제시되지 않은 창의적인 아이디어를 자유롭게 시도해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 전략: 1. 명확한 지침 작성 \n",
    "#### 전술: 1. 관련성 높은 답변을 얻으려면 문의에 세부 정보를 포함하세요 \n",
    "관련성 높은 답변을 얻으려면 요청에 중요한 세부 정보나 맥락이 포함되어 있는지 확인하세요. 그렇지 않으면 모델이 무슨 뜻인지 추측할 수밖에 없습니다.\n",
    "\n",
    "|Worse|Better|\n",
    "|---|---|\n",
    "|Excel에서 숫자를 더하려면 어떻게 하나요?|Excel에서 달러 금액의 행을 합산하려면 어떻게 하나요? 모든 합계가 '합계'라는 열에서 오른쪽으로 끝나는 전체 행에 대해 자동으로 이 작업을 수행하고 싶습니다.  |\n",
    "|대통령은 누구인가요?| 2021년 멕시코의 대통령은 누구이며 선거는 얼마나 자주 실시되나요? |\n",
    "|피보나치 수열을 계산하는 코드를 작성하세요. | 피보나치 수열을 효율적으로 계산하는 TypeScript 함수를 작성하세요. 코드에 자유롭게 주석을 달아 각 부분이 무엇을 하고 왜 그렇게 작성되었는지 설명하세요.|\n",
    "| 회의 노트를 요약하세요.| 회의 노트를 한 단락으로 요약하세요. 그런 다음 발표자와 각 발표자의 핵심 요점에 대한 마크다운 목록을 작성합니다. 마지막으로 발표자가 제안한 다음 단계 또는 실행 항목이 있으면 나열합니다.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#install langchain-openai\n",
    "\n",
    "%pip install -qU langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce1f85f3463d4d89a0610fe1826decb5 https://AOAI-GPT-byungsukim.openai.azure.com/ gpt-4o-global 2024-05-01-preview\n"
     ]
    }
   ],
   "source": [
    "#This basic example to initialize the Azure OpenAI model using langchain-openai\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Retrieve Azure OpenAI specific configuration from environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "OPENAI_API_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "OPENAI_DEPLOYMENT_MODEL = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "print (OPENAI_API_KEY, OPENAI_API_ENDPOINT, OPENAI_DEPLOYMENT_MODEL, OPENAI_API_VERSION)\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=OPENAI_DEPLOYMENT_MODEL,\n",
    "    azure_endpoint= OPENAI_API_ENDPOINT,\n",
    "    openai_api_type=\"azure\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    api_version=OPENAI_API_VERSION,\n",
    "    temperature=0.5,\n",
    "    max_tokens=None,\n",
    "    timeout = None,\n",
    "    max_retries = 2,\n",
    "    # other params....\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재의 대통령이 누구인지는 제가 최신 정보를 제공할 수 없는 상황이기 때문에, 특정 국가와 시점을 명시해 주셔야 정확한 답변을 드릴 수 있습니다. 예를 들어, \"2023년 대한민국의 대통령은 누구인가요?\"와 같이 질문해 주시면 도움이 됩니다. 최신 정보는 뉴스나 공식 웹사이트를 통해 확인해 주세요.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "message= HumanMessage(\n",
    "    content= \"대통령은 누구인가요?\"\n",
    "    )\n",
    "ai_msg = llm.invoke([message])\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021년 멕시코의 대통령은 안드레스 마누엘 로페스 오브라도르(Andrés Manuel López Obrador)입니다. 그는 2018년 12월 1일부터 대통령직을 수행하고 있습니다. 멕시코의 대통령 선거는 6년마다 실시되며, 대통령은 한 번의 임기만을 수행할 수 있습니다. 따라서 재선은 허용되지 않습니다.\n"
     ]
    }
   ],
   "source": [
    "message= HumanMessage(\n",
    "    content= \"2021년 멕시코의 대통령은 누구이며 선거는 얼마나 자주 실시되나요?\"\n",
    "    )\n",
    "ai_msg = llm.invoke([message])\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전술: 2. 모델에게 페르소나를 채택하도록 요청하기 \n",
    "시스템 메시지를 사용하여 모델이 답장에 사용하는 페르소나를 지정할 수 있습니다. 대형언어모델의 시스템 메시지를 이용하여 대형언어모델의 응답방향을 조절할 수 있습니다. \n",
    "\n",
    "|역할|프롬프트|\n",
    "|------|-----|\n",
    "|시스템|사용자가 문서 작성에 도움을 요청하면 모든 단락에 농담이나 장난스러운 댓글이 하나 이상 포함된 문서로 응답합니다.|\n",
    "|사용자|제 강철 볼트 공급업체에 짧은 시간 내에 납품을 완료해 준 것에 대한 감사 편지를 작성해주세요. 덕분에 중요한 주문을 배송할 수 있었습니다.|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요 [공급업체 이름] 팀,\n",
      "\n",
      "이메일을 통해 진심으로 감사의 인사를 전하고자 합니다. 귀사의 신속하고 효율적인 서비스 덕분에 저희는 중요한 주문을 제 시간에 배송할 수 있었습니다. 정말로, 귀사의 도움 없이는 불가능했을 것입니다. (그리고 솔직히 말해, 저희는 강철 볼트가 슈퍼히어로처럼 날아다니는 꿈을 꾸기도 했습니다!)\n",
      "\n",
      "귀사의 뛰어난 서비스와 협력 덕분에 저희는 고객들에게 최고의 품질과 신뢰성을 제공할 수 있었습니다. 짧은 시간 내에 납품을 완료해 주신 덕분에 저희 프로젝트는 순조롭게 진행될 수 있었고, 이는 모두 귀사의 노력 덕분입니다. (혹시 귀사에 시간 여행 기계가 있는 건 아니죠? 그렇다면 저희도 하나 부탁드립니다!)\n",
      "\n",
      "앞으로도 계속해서 귀사와의 협력을 기대하며, 언제나 믿을 수 있는 파트너로 남아주시길 바랍니다. 다시 한 번 진심으로 감사드립니다. (그리고 만약 다음 번에 강철 볼트가 날아다니는 걸 보게 된다면, 저희는 귀사를 의심하지 않을게요!)\n",
      "\n",
      "감사합니다.\n",
      "\n",
      "[당신의 이름]\n",
      "[당신의 직책]\n",
      "[당신의 회사 이름]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "system_msg = SystemMessage(content=\"사용자가 문서 작성에 도움을 요청하면 모든 단락에 농담이나 장난스러운 댓글이 하나 이상 포함된 문서로 응답합니다.\")\n",
    "human_msg = HumanMessage(content=\"제 강철 볼트 공급업체에 짧은 시간 내에 납품을 완료해 준 것에 대한 감사 편지를 작성해주세요. 덕분에 중요한 주문을 배송할 수 있었습니다.\")\n",
    "\n",
    "response = llm.invoke([system_msg,human_msg])\n",
    "\n",
    "answers = response.content\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전술 4 : 구분 기호를 사용하여 입력의 다른 부분을 명확하게 표시하기 \n",
    "\n",
    "큰따옴표, XML 태그, 섹션 제목 등과 같은 구분 기호를 사용하면 텍스트의 섹션을 구분하여 다르게 처리하는 데 도움이 될 수 있습니다. \n",
    "\n",
    "|역할|프롬프트|\n",
    "|---|---|\n",
    "|USER|3연속 큰따옴표로 구분된 텍스트를 시조로 요약하세요.\"\"\"위메이드는 성장의 가치, 책임의 가치 그리고 성취의 가치를 중요시합니다. 누군가의 꿈이 현실로 이루어지면서 세상은 더 좋은 곳으로 발전하고 있습니다. 게임에서 시작하여 다양한 문화의 영역에서 무한한 상상을 현실로 만드는 창조집단, 그것이 위메이드의 비전입니다.\"\"\"|\n",
    "\n",
    "<br>\n",
    "\n",
    "|역할|프롬프트|\n",
    "|---|---|\n",
    "|SYSTEM|동일한 주제에 대한 한 쌍의 문서(XML 태그로 구분됨)가 제공됩니다. 먼저 각 기사의 주장을 요약하세요. 그런 다음 어느 쪽이 더 나은 주장인지 표시하고 그 이유를 설명하세요.|\n",
    "|USER |\\<article> 국내 게임사 중 적극적으로 블록체인 사업을 추진해 온 위메이드가 최근 관련 서비스를 축소하면서 '선택과 집중'을 키워드로 내세웠다.실적 개선 기대에도 주춤했던 위메이드 주가, 가상자산 위믹스가 위믹스 재단의 대대적인 변화로 상승세로 전환할지 주목된다.27일 업계에 따르면 위메이드는 최근 위믹스의 토크노믹스를 전면 개편하겠다고 밝혔다. 이에 따라 7월 1일부터 위믹스3.0 메인넷에 새 토크노믹스 '브리오슈 하드포크'를 적용한다. 반감기가 도입되며 블록당 1개씩 발행되는 '위믹스' 민팅 규칙은 16차례의 업데이트를 거쳐 감소시키는 것이 핵심이다. 재단이 보유한 물량 중 약 4억3500만개가 소각될 예정이다. 남은 물량은 유통 계획에 따라 생태계 발전 기금이나 개발비 등으로 분배할 예정이다. 위믹스 가치를 제고하고, 커뮤니티와의 동반 성장을 도모한다는 계획이다. 이와 함께 위믹스 기반의 블록체인 게임 플랫폼 '위믹스 플레이'도 새롭게 개편된다. 현재 개발 중인 다중접속역할수행게임(MMORPG) '레전드 오브 이미르'에도 블록체인 기술을 접목한다고 밝혔다. 지난 3월 '위믹스의 아버지'로 불리던 장현국 대표가 사임하고 창업주인 박관호 대표가 경영 전면에 나선 이후 위메이드는 블록체인 사업을 정리해 위믹스 가격이 약세를 나타냈다. 위메이드는 최근 '미르' 지식재산권(IP)에 위믹스 기반 토큰 경제를 적용한 '미르M' 글로벌 서비스를 종료했다. 상대적으로 매출에 기여하지 못하는 '미르M'의 서비스를 접고 블록체인 게임 매출을 견인하는 '나이트 크로우'에 집중하겠다는 의도로 풀이된다. 이밖에도 가상자산 지급 '플레이월렛'의 한국 서비스와 탈중앙화금융(디파이) '위믹스 커런시' 등의 서비스도 종료했다. 블록체인 사업 기조가 다소 축소되면서 위믹스는 좀처럼 반등 기미를 보이지 않았다. 위믹스 반감기 도입과 물량 소각 등 토크노믹스 개편 소식에 전날 위믹스 가격이 급등하긴 했지만 여전히 지난해 12월 5300원대를 기록한 것엔 미치지 못한다. 지난 26일 오후 3시30분 기준 위믹스 가격은 전날 보다 18.1% 오른 1905원대를 기록했다. 주가도 비슷한 흐름을 보인다. 위믹스 가격이 급등했던 지난 3월 위메이드 주가는 7만6100원(3월21일 종가)까지 올랐지만 이후 우하향 곡선을 그렸다. 지난 26일 종가 기준 4만2600원을 기록하면서 3개월새 약 44% 하락했다. 박 대표는 지난 5월 1분기 실적발표 컨퍼런스 콜에서 \"조직 구조 재편, 리스크 관리 강화와 비용 최적화를 통해 수익화 중심 블록체인 사업 확장 전략을 추진함으로써 위믹스 생태계와 위메이드의 장기적인 성장을 극대화하겠다\"고 밝혔다. \\</article> <br> \\<article> 위메이드가 '선택과 집중'을 통한 효율성 강화에 나선다. 지난 3월 12년 만에 경영 일선에 복귀한 박관호 대표는 게임 산업에 집중하며 반등을 이끌고 있다. 게임 관련 매출 성장세가 예상되는 가운데 비용 관리가 위메이드의 실적을 좌우할 것으로 보인다. 8일 업계에 따르면 위메이드는 신사업 확장으로 인한 비용 증가로 오랜 기간 적자를 기록하고 있다. 2019년 1136억 원이었던 위메이드의 매출은 2022년 4635억 원, 지난해 6053억 원 까지 성장했다. 하지만, 2022년과 2023년 각각 영업 손실 약 1310억 원, 1570억 원을 기록하며 적자 폭은 깊어지고 있다. 이를 개선하기 위해 위메이드는 수익성이 적은 사업 정리에 나서고 있다. 실제, 박 대표는 지난 3월 개최된 정기주주총회에서 \"적자가 커 회사 비용을 최적화 해야 한다\"고 언급한 바 있다. 위메이드는 미르M 국내외 서비스를 종료하겠다고 밝혔으며 영업 비용이 많이 발생했던 블록체인 사업 교통정리에도 나서고 있다. 박 대표가 취임한 이후 종료 및 축소 공지를 내린 블록체인 서비스만 10여개나 된다.업계는 위메이드가 게임 산업에 집중하기 위해 이런 전략을 구성했다고 평가했다. 업계 관계자는 \"세계적으로 블록체인·NFT(대체불가토큰) 관련 사업에 대한 기대감이 떨어지고 있다\"며 \":이런 상황에서 위메이드가 선택과 집중을 가져가는 것은 자연스러운 현상\"이라고 말했다. 다행히 위메이드의 게임 사업 분위기는 좋은 상황이다. 지난 1분기 출시된 '나이트크로우' 글로벌 버전이 론칭 3일 만에 누적 매출 1000만 달러(이날 기준138억1700만 원)을 기록하는 등 호성적을 기록했다. 또 하반기 기대작 '레전드 오브 이미르'가 출시될 예정이다. 미르4와 미르M 중국 출시도 위메이드 성적 개선에 키포인트다. 위메이드가 2001년 부터 중국에서 '미르의 전설2'를 서비스했던 역량을 바탕으로 중국 시장 선점에 나설 것으로 예상된다. 2020년 보스턴컨설팅그룹(BCG)에 따르면 중국 내 미르 IP의 시장 규모는 대해 연간 9조 원으로 평가받는다. 증권가는 위메이드가 이런 전략에 힘입어 3분기부터 실적 회복에 나설 것이라고 내다봤다. NH 투자증권에 따르면 위메이드는 3분기부터 영업이익 99억7000억 원을 기록하며 흑자전환할 것으로 예상된다.  한편, 위메이드는 블록체인 게임 플랫폼 '위믹스 플레이(WEMIX PLAY)의 개편을 진행한다. 이를 위해 오는 16일 '위믹스 데이'를 개최하고 관련 내용을 공유한다. 위믹스 데이에서는 새롭게 선보일 고도화된 블록체인 서비스와 토크노믹스가 처음으로 공개된다. 또 블록체인 게임 이용자, 커뮤니티 등 위믹스 생태계 전반이 함께 성장할 수 있도록, 토크노믹스를 강화하고 고품질 게임을 온보딩해 커뮤니티 중심 플랫폼으로 변화할 예정이다. 위메이드 관계자는 \"위믹스 플레이와 위믹스 리퍼블릭 중심으로 위믹스 생태계를 키워나갈 예정\"이라며 \"현재 서비스 중인 게임들과 앞으로 다가오는 신작 흥행에도 집중해 호성적을 기록할 것\"이라고 말했다.  \\</article>|\n",
    "\n",
    "<br>\n",
    "\n",
    "|역할|프롬프트|\n",
    "|---|---|\n",
    "|SYSTEM|논문 초록과 제안된 논문 제목이 제공됩니다. 논문 제목은 독자가 논문의 주제를 잘 파악할 수 있으면서도 눈길을 끌 수 있어야 합니다. 제목이 이러한 기준을 충족하지 않는 경우 5가지 대안을 제안하세요.|\n",
    "|USER| 초록: 최근 생성형 AI 기술의 발전으로 인해 대형 언어 모델(Large Language Model, LLM)의 활용 및 도입이 확대되고 있는 상황에서 기존 연구들은 기업내부 데이터의 활용에 대한 실제 적용사례나 구현방법을 찾아보기 힘들다. 이에 따라 본 연구에서는 가장 많이 이용되고 있는 LangChain 프레임워크를 이용한 LLM 애플리케이션 아키텍처를 활용하여 생성형 AI 서비스를 구현하는 방법을 제시한다. 이를 위해 LLM의 활용을 중심으로, 정보 부족 문제를 극복하는 다양한 방법을 검토하고 구체적인 해결책을 제시하였다. 이를 위해 파인튜닝이나 직접 문서 정보를 활용하는 방법을 분석하며, 이러한 문제를 해결하기 위한 RAG 모델을 활용한 정보 저장 및 검색 방법에 대해 주요단계에 대해 자세하게 살펴본다. 특히, RAG 모델을 활용하여 정보를 벡터저장소에 저장하고 검색하기 위한 방법으로 유사문맥 추천 및 QA시스템을 활용하였다. 또한 구체적인 작동 방식과 주요한 구현 단계 및 사례를 구현소스 및 사용자 인터페이스까지 제시하여 생성형 AI 기술에 대한 이해를 높였다. 이를 통해 LLM을 활용한 기업내 서비스 구현에 적극적으로 활용할 수 있도록 하는데 의미와 가치가 있다.<br> 제목: LLM 애플리케이션 아키텍처를 활용한 생성형 AI 서비스 구현: RAG모델과 LangChain 프레임워크 기반 |\n",
    "\n",
    "이와 같이 간단한 작업의 경우 구분 기호를 사용해도 출력 품질에 차이가 없을 수 있습니다. 하지만 작업이 복잡할수록 작업 세부 사항을 명확히 구분하는 것이 더 중요합니다. 모델에게 무엇을 요구하는지 정확히 이해하기 위해 모델이 고민하게 하지 마세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "위메이드의 꿈과 비전,\n",
      "성장과 책임, 성취로\n",
      "세상 더 나아진다.\n"
     ]
    }
   ],
   "source": [
    "user_msg = HumanMessage(content='3연속 큰따옴표로 구분된 텍스트를 시조로 요약하세요.\"\"\"위메이드는 성장의 가치, 책임의 가치 그리고 성취의 가치를 중요시합니다. 누군가의 꿈이 현실로 이루어지면서 세상은 더 좋은 곳으로 발전하고 있습니다. 게임에서 시작하여 다양한 문화의 영역에서 무한한 상상을 현실로 만드는 창조집단, 그것이 위메이드의 비전입니다.\"\"\"')\n",
    "response = llm.invoke([user_msg])\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제공된 제목이 논문의 주제를 잘 반영하고 있지만, 더 눈길을 끌 수 있는 대안을 몇 가지 제안드립니다:\n",
      "\n",
      "1. \"LangChain과 RAG 모델로 구현한 생성형 AI 서비스: 기업 내 데이터 활용 사례\"\n",
      "2. \"기업 데이터를 활용한 생성형 AI: LangChain 프레임워크와 RAG 모델 적용법\"\n",
      "3. \"생성형 AI 서비스 구현 전략: LangChain과 RAG 모델을 통한 기업 데이터 활용\"\n",
      "4. \"LangChain과 RAG 모델을 통한 LLM 기반 생성형 AI 서비스 구현 방법\"\n",
      "5. \"기업 데이터와 생성형 AI: LangChain 프레임워크와 RAG 모델을 활용한 실전 가이드\"\n"
     ]
    }
   ],
   "source": [
    "system_msg= SystemMessage(content=\"\"\"\n",
    "논문 초록과 제안된 논문 제목이 제공됩니다. 논문 제목은 독자가 논문의 주제를 잘 파악할 수 있으면서도 눈길을 끌 수 있어야 합니다. 제목이 이러한 기준을 충족하지 않는 경우 5가지 대안을 제안하세요.\n",
    "\"\"\")\n",
    "user_msg = HumanMessage(content=\"\"\"\n",
    "초록: 최근 생성형 AI 기술의 발전으로 인해 대형 언어 모델(Large Language Model, LLM)의 활용 및 도입이 확대되고 있는 상황에서 기존 연구들은 기업내부 데이터의 활용에 대한 실제 적용사례나 구현방법을 찾아보기 힘들다. 이에 따라 본 연구에서는 가장 많이 이용되고 있는 LangChain 프레임워크를 이용한 LLM 애플리케이션 아키텍처를 활용하여 생성형 AI 서비스를 구현하는 방법을 제시한다. 이를 위해 LLM의 활용을 중심으로, 정보 부족 문제를 극복하는 다양한 방법을 검토하고 구체적인 해결책을 제시하였다. 이를 위해 파인튜닝이나 직접 문서 정보를 활용하는 방법을 분석하며, 이러한 문제를 해결하기 위한 RAG 모델을 활용한 정보 저장 및 검색 방법에 대해 주요단계에 대해 자세하게 살펴본다. 특히, RAG 모델을 활용하여 정보를 벡터저장소에 저장하고 검색하기 위한 방법으로 유사문맥 추천 및 QA시스템을 활용하였다. 또한 구체적인 작동 방식과 주요한 구현 단계 및 사례를 구현소스 및 사용자 인터페이스까지 제시하여 생성형 AI 기술에 대한 이해를 높였다. 이를 통해 LLM을 활용한 기업내 서비스 구현에 적극적으로 활용할 수 있도록 하는데 의미와 가치가 있다.\n",
    "제목: LLM 애플리케이션 아키텍처를 활용한 생성형 AI 서비스 구현: RAG모델과 LangChain 프레임워크 기반\n",
    "\"\"\")\n",
    "\n",
    "response = llm.invoke([system_msg,user_msg])\n",
    "\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
