{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering \n",
    "\n",
    "이 가이드에서는 GPT-4o와 같은 대규모 언어 모델(GPT 모델이라고도 함)에서 더 나은 결과를 얻기 위한 전략과 전술을 공유합니다. 여기에 설명된 방법들은 때때로 더 큰 효과를 위해 조합하여 배포할 수 있습니다. 자신에게 가장 적합한 방법을 찾기 위해 실험해 보시기 바랍니다. 또한 모델 기능을 보여주는 예시 프롬프트도 살펴볼 수 있습니다:\n",
    "\n",
    "## 더 나은 결과를 얻기 위한 6가지 전략\n",
    "\n",
    "### 1. 명확한 지침 작성\n",
    "대형언어모델은 사용자의 마음을 읽을 수 없습니다. 답변이 너무 길면 간단한 답변을 요청하세요. 답변이 너무 간단하다면 전문가 수준의 문장을 요청하세요. 형식이 마음에 들지 않으면 원하는 형식을 직접 보여주세요. 모델이 원하는 것을 추측할 필요가 적을수록 원하는 것을 얻을 가능성이 높아집니다.\n",
    "\n",
    "#### 방안:\n",
    "\n",
    "1. 쿼리에 세부 정보를 포함하여 보다 관련성 높은 답변 얻기\n",
    "2. 모델에 페르소나를 채택하도록 요청하기 \n",
    "3. 구분 기호를 사용하여 입력의 구분되는 부분을 명확하게 표시하기 \n",
    "4. 작업 완료에 필요한 단계 지정하기 \n",
    "5. 예제 제공 \n",
    "6. 출력의 원하는 길이 지정하기\n",
    "\n",
    "### 2. 참조 텍스트 제공\n",
    "언어 모델은 특히 난해한 주제나 인용 및 URL에 대한 질문을 받을 때 자신 있게 가짜 답변을 만들어낼 수 있습니다. 노트 한 장이 시험 성적을 높이는 데 도움이 되는 것과 마찬가지로, 이러한 모델에 참조 텍스트를 제공하면 더 적은 조작으로 답안을 작성하는 데 도움이 될 수 있습니다.\n",
    "\n",
    "#### 방안:\n",
    "\n",
    "1. 모델에게 참조 텍스트를 사용하여 답하도록 지시하기 \n",
    "2. 모델에게 참조 텍스트의 인용을 사용하여 답하도록 지시하기\n",
    "\n",
    "### 3. 복잡한 작업을 더 간단한 하위 작업으로 나누기 \n",
    "소프트웨어 엔지니어링에서 복잡한 시스템을 일련의 모듈식 구성 요소로 분해하는 것이 좋은 관행인 것처럼 언어 모델에 제출된 작업도 마찬가지입니다. 복잡한 작업은 단순한 작업보다 오류율이 높은 경향이 있습니다. 또한 복잡한 작업은 종종 이전 작업의 출력을 사용하여 이후 작업의 입력을 구성하는 더 간단한 작업의 워크플로로 재정의할 수 있습니다.\n",
    "\n",
    "#### 방안:\n",
    "\n",
    "1. 의도 분류를 사용하여 사용자 쿼리에 가장 관련성이 높은 지침 식별 \n",
    "2. 매우 긴 대화가 필요한 대화 애플리케이션의 경우 이전 대화를 요약하거나 필터링 \n",
    "3. 긴 문서를 부분적으로 요약하고 전체 요약을 재귀적으로 구성\n",
    "\n",
    "### 4.모델에 '생각할 시간' 주기 \n",
    "\n",
    "17에 28을 곱하라는 질문을 받으면 즉시 알 수는 없지만 시간이 지나면 풀 수 있습니다. 마찬가지로 모델은 시간을 들여 답을 찾는 것보다 바로 답을 찾으려고 할 때 더 많은 추론 오류를 범합니다. 답을 내기 전에 '생각의 연쇄(Chain of Thought)'를 요청하면 모델이 보다 안정적으로 정답을 추론하는 데 도움이 될 수 있습니다.\n",
    "\n",
    "#### 방안:\n",
    "\n",
    "1. 결론을 내리기 전에 모델 스스로 해결책을 찾도록 지시하기 \n",
    "2. 모델의 추론 과정을 숨기기 위해 내부 독백 또는 일련의 쿼리를 사용하기 \n",
    "3. 모델에게 이전 통과에서 놓친 것이 있는지 물어보기\n",
    "\n",
    "### 5. 외부 도구 사용 \n",
    "다른 도구의 출력을 모델에 공급하여 모델의 약점을 보완하세요. 예를 들어, 텍스트 검색 시스템(RAG 또는 검색 증강 생성이라고도 함)은 모델에 관련 문서에 대해 알려줄 수 있습니다. 코드 인터프리터와 같은 코드 실행 엔진은 모델이 수학을 수행하고 코드를 실행하는 데 도움을 줄 수 있습니다. 언어 모델보다 도구로 더 안정적이고 효율적으로 작업을 수행할 수 있다면, 그 작업을 오프로드하여 두 가지 장점을 모두 활용하세요.\n",
    "\n",
    "#### 방안:\n",
    "\n",
    "1. 임베딩 기반 검색을 사용하여 효율적인 지식 검색 구현 \n",
    "2. 코드 실행을 사용하여 보다 정확한 계산을 수행하거나 외부 API 호출 \n",
    "3. 모델에 특정 기능에 대한 액세스 권한 부여\n",
    "\n",
    "### 6. 체계적으로 변경 사항 테스트하기 \n",
    "성능을 측정할 수 있다면 성능 개선이 더 쉬워집니다. 프롬프트를 수정하면 일부 고립된 예제에서는 성능이 향상되지만 보다 대표적인 예제 집합에서는 전반적인 성능이 저하되는 경우가 있습니다. 따라서 변경 사항이 성능에 긍정적인 영향을 미치는지 확인하려면 포괄적인 테스트 모음('평가'라고도 함)을 정의해야 할 수도 있습니다.\n",
    "\n",
    "#### 방안:\n",
    "\n",
    "1. 표준 답변을 참조하여 모델 결과 평가하기\n",
    "\n",
    "## 맺으며\n",
    "위에 나열된 각 전략들은 구체적인 방안들로 구현할 수 있으며 조합하여 사용할 수 있습니다. 이러한 방안들은 시도해 볼 수 있는 아이디어를 제공하기 위한 것으로 모든 경우를 완전히 포괄하는 것은 아니므로 여기에 제시되지 않은 창의적인 아이디어를 자유롭게 시도해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 전략 1. 명확한 지침 작성 \n",
    "#### 방안 1. 관련성 높은 답변을 얻으려면 문의에 세부 정보를 포함하세요 \n",
    "관련성 높은 답변을 얻으려면 요청에 중요한 세부 정보나 맥락이 포함되어 있는지 확인하세요. 그렇지 않으면 모델이 무슨 뜻인지 추측할 수밖에 없습니다.\n",
    "\n",
    "|Worse|Better|\n",
    "|---|---|\n",
    "|Excel에서 숫자를 더하려면 어떻게 하나요?|Excel에서 달러 금액의 행을 합산하려면 어떻게 하나요? 모든 합계가 '합계'라는 열에서 오른쪽으로 끝나는 전체 행에 대해 자동으로 이 작업을 수행하고 싶습니다.  |\n",
    "|대통령은 누구인가요?| 2021년 멕시코의 대통령은 누구이며 선거는 얼마나 자주 실시되나요? |\n",
    "|피보나치 수열을 계산하는 코드를 작성하세요. | 피보나치 수열을 효율적으로 계산하는 TypeScript 함수를 작성하세요. 코드에 자유롭게 주석을 달아 각 부분이 무엇을 하고 왜 그렇게 작성되었는지 설명하세요.|\n",
    "| 회의 노트를 요약하세요.| 회의 노트를 한 단락으로 요약하세요. 그런 다음 발표자와 각 발표자의 핵심 요점에 대한 마크다운 목록을 작성합니다. 마지막으로 발표자가 제안한 다음 단계 또는 실행 항목이 있으면 나열합니다.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#install langchain-openai\n",
    "\n",
    "%pip install -qU langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deb7f6186128481aad7f7b4a3c2ee4fa https://AOAI-GPT-byungsukim.openai.azure.com/ gpt-4o-global 2024-05-01-preview\n"
     ]
    }
   ],
   "source": [
    "#This basic example to initialize the Azure OpenAI model using langchain-openai\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Retrieve Azure OpenAI specific configuration from environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "OPENAI_API_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "OPENAI_DEPLOYMENT_MODEL = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "print (OPENAI_API_KEY, OPENAI_API_ENDPOINT, OPENAI_DEPLOYMENT_MODEL, OPENAI_API_VERSION)\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=OPENAI_DEPLOYMENT_MODEL,\n",
    "    azure_endpoint= OPENAI_API_ENDPOINT,\n",
    "    openai_api_type=\"azure\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    api_version=OPENAI_API_VERSION,\n",
    "    temperature=0.5,\n",
    "    max_tokens=None,\n",
    "    timeout = None,\n",
    "    max_retries = 2,\n",
    "    # other params....\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제가 현재 시점에 대한 실시간 정보를 제공할 수는 없지만, 2021년 기준으로 대한민국의 대통령은 문재인입니다. 만약 최신 정보를 원하신다면, 인터넷 검색이나 뉴스 매체를 통해 확인해 보시기 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "message= HumanMessage(\n",
    "    content= \"대통령은 누구인가요?\"\n",
    "    )\n",
    "ai_msg = llm.invoke([message])\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021년 멕시코의 대통령은 안드레스 마누엘 로페스 오브라도르(Andrés Manuel López Obrador)입니다. 그는 2018년 12월 1일에 취임했습니다.\n",
      "\n",
      "멕시코의 대통령 선거는 6년마다 실시됩니다. 대통령은 한 번의 임기만을 수행할 수 있으며, 재선은 허용되지 않습니다. 따라서 다음 대통령 선거는 2024년에 예정되어 있습니다.\n"
     ]
    }
   ],
   "source": [
    "message= HumanMessage(\n",
    "    content= \"2021년 멕시코의 대통령은 누구이며 선거는 얼마나 자주 실시되나요?\"\n",
    "    )\n",
    "ai_msg = llm.invoke([message])\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 방안 2. 모델에게 페르소나를 채택하도록 요청하기 \n",
    "시스템 메시지를 사용하여 모델이 답장에 사용하는 페르소나를 지정할 수 있습니다. 대형언어모델의 시스템 메시지를 이용하여 대형언어모델의 응답방향을 조절할 수 있습니다. \n",
    "\n",
    "|역할|프롬프트|\n",
    "|------|-----|\n",
    "|SYSTEM|사용자가 문서 작성에 도움을 요청하면 모든 단락에 농담이나 장난스러운 댓글이 하나 이상 포함된 문서로 응답합니다.|\n",
    "|USER|제 강철 볼트 공급업체에 짧은 시간 내에 납품을 완료해 준 것에 대한 감사 편지를 작성해주세요. 덕분에 중요한 주문을 배송할 수 있었습니다.|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요 [공급업체 이름] 팀,\n",
      "\n",
      "안녕하세요 [공급업체 이름] 팀,\n",
      "\n",
      "우선, 신속하고 효율적인 납품에 대해 진심으로 감사드립니다. 여러분 덕분에 저희는 중요한 주문을 제때에 배송할 수 있었습니다. 정말이지, 여러분의 속도는 번개보다 빠르군요! 혹시 슈퍼히어로 팀을 운영하고 계신 건 아닌지 궁금할 정도입니다.\n",
      "\n",
      "저희는 항상 여러분의 품질 높은 강철 볼트와 탁월한 서비스에 감탄하고 있습니다. 이번에도 예외는 아니었고, 여러분의 빠른 대응 덕분에 저희 고객들도 매우 만족해하고 있습니다. 사실, 저희 고객이 여러분의 볼트를 보고 \"이건 진짜 볼트 같은 볼트다!\"라고 감탄했답니다.\n",
      "\n",
      "앞으로도 계속해서 좋은 협력 관계를 유지하며 함께 성장해 나가기를 기대합니다. 다음번에도 이렇게 빠른 납품이 필요할 때, 여러분께 다시 한 번 부탁드릴 수 있기를 바랍니다. 물론, 번개보다 빠른 납품은 항상 환영입니다!\n",
      "\n",
      "다시 한 번 진심으로 감사드리며, 여러분의 팀에게도 최고의 하루가 되기를 바랍니다. 그리고 혹시 볼트가 필요하면 언제든지 연락 주세요. 저희는 언제나 준비되어 있습니다!\n",
      "\n",
      "감사합니다.\n",
      "\n",
      "[당신의 이름]\n",
      "[당신의 직책]\n",
      "[당신의 회사 이름]\n",
      "\n",
      "P.S. 다음번에는 볼트와 함께 번개 배달 서비스도 추가해 주실 수 있을까요? 😄\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "system_msg = SystemMessage(content=\"사용자가 문서 작성에 도움을 요청하면 모든 단락에 농담이나 장난스러운 댓글이 하나 이상 포함된 문서로 응답합니다.\")\n",
    "human_msg = HumanMessage(content=\"제 강철 볼트 공급업체에 짧은 시간 내에 납품을 완료해 준 것에 대한 감사 편지를 작성해주세요. 덕분에 중요한 주문을 배송할 수 있었습니다.\")\n",
    "\n",
    "response = llm.invoke([system_msg,human_msg])\n",
    "\n",
    "answers = response.content\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 방안 3 : 구분 기호를 사용하여 입력의 다른 부분을 명확하게 표시하기 \n",
    "\n",
    "큰따옴표, XML 태그, 섹션 제목 등과 같은 구분 기호를 사용하면 텍스트의 섹션을 구분하여 다르게 처리하는 데 도움이 될 수 있습니다. \n",
    "\n",
    "|역할|프롬프트|\n",
    "|---|---|\n",
    "|USER|3연속 큰따옴표로 구분된 텍스트를 시조로 요약하세요.\"\"\"위메이드는 성장의 가치, 책임의 가치 그리고 성취의 가치를 중요시합니다. 누군가의 꿈이 현실로 이루어지면서 세상은 더 좋은 곳으로 발전하고 있습니다. 게임에서 시작하여 다양한 문화의 영역에서 무한한 상상을 현실로 만드는 창조집단, 그것이 위메이드의 비전입니다.\"\"\"|\n",
    "\n",
    "<br>\n",
    "\n",
    "|역할|프롬프트|\n",
    "|---|---|\n",
    "|SYSTEM|동일한 주제에 대한 한 쌍의 문서(XML 태그로 구분됨)가 제공됩니다. 먼저 각 기사의 주장을 요약하세요. 그런 다음 어느 쪽이 더 나은 주장인지 표시하고 그 이유를 설명하세요.|\n",
    "|USER |\\<article> 국내 게임사 중 적극적으로 블록체인 사업을 추진해 온 위메이드가 최근 관련 서비스를 축소하면서 '선택과 집중'을 키워드로 내세웠다.실적 개선 기대에도 주춤했던 위메이드 주가, 가상자산 위믹스가 위믹스 재단의 대대적인 변화로 상승세로 전환할지 주목된다.27일 업계에 따르면 위메이드는 최근 위믹스의 토크노믹스를 전면 개편하겠다고 밝혔다. 이에 따라 7월 1일부터 위믹스3.0 메인넷에 새 토크노믹스 '브리오슈 하드포크'를 적용한다. 반감기가 도입되며 블록당 1개씩 발행되는 '위믹스' 민팅 규칙은 16차례의 업데이트를 거쳐 감소시키는 것이 핵심이다. 재단이 보유한 물량 중 약 4억3500만개가 소각될 예정이다. 남은 물량은 유통 계획에 따라 생태계 발전 기금이나 개발비 등으로 분배할 예정이다. 위믹스 가치를 제고하고, 커뮤니티와의 동반 성장을 도모한다는 계획이다. 이와 함께 위믹스 기반의 블록체인 게임 플랫폼 '위믹스 플레이'도 새롭게 개편된다. 현재 개발 중인 다중접속역할수행게임(MMORPG) '레전드 오브 이미르'에도 블록체인 기술을 접목한다고 밝혔다. 지난 3월 '위믹스의 아버지'로 불리던 장현국 대표가 사임하고 창업주인 박관호 대표가 경영 전면에 나선 이후 위메이드는 블록체인 사업을 정리해 위믹스 가격이 약세를 나타냈다. 위메이드는 최근 '미르' 지식재산권(IP)에 위믹스 기반 토큰 경제를 적용한 '미르M' 글로벌 서비스를 종료했다. 상대적으로 매출에 기여하지 못하는 '미르M'의 서비스를 접고 블록체인 게임 매출을 견인하는 '나이트 크로우'에 집중하겠다는 의도로 풀이된다. 이밖에도 가상자산 지급 '플레이월렛'의 한국 서비스와 탈중앙화금융(디파이) '위믹스 커런시' 등의 서비스도 종료했다. 블록체인 사업 기조가 다소 축소되면서 위믹스는 좀처럼 반등 기미를 보이지 않았다. 위믹스 반감기 도입과 물량 소각 등 토크노믹스 개편 소식에 전날 위믹스 가격이 급등하긴 했지만 여전히 지난해 12월 5300원대를 기록한 것엔 미치지 못한다. 지난 26일 오후 3시30분 기준 위믹스 가격은 전날 보다 18.1% 오른 1905원대를 기록했다. 주가도 비슷한 흐름을 보인다. 위믹스 가격이 급등했던 지난 3월 위메이드 주가는 7만6100원(3월21일 종가)까지 올랐지만 이후 우하향 곡선을 그렸다. 지난 26일 종가 기준 4만2600원을 기록하면서 3개월새 약 44% 하락했다. 박 대표는 지난 5월 1분기 실적발표 컨퍼런스 콜에서 \"조직 구조 재편, 리스크 관리 강화와 비용 최적화를 통해 수익화 중심 블록체인 사업 확장 전략을 추진함으로써 위믹스 생태계와 위메이드의 장기적인 성장을 극대화하겠다\"고 밝혔다. \\</article> <br> \\<article> 위메이드가 '선택과 집중'을 통한 효율성 강화에 나선다. 지난 3월 12년 만에 경영 일선에 복귀한 박관호 대표는 게임 산업에 집중하며 반등을 이끌고 있다. 게임 관련 매출 성장세가 예상되는 가운데 비용 관리가 위메이드의 실적을 좌우할 것으로 보인다. 8일 업계에 따르면 위메이드는 신사업 확장으로 인한 비용 증가로 오랜 기간 적자를 기록하고 있다. 2019년 1136억 원이었던 위메이드의 매출은 2022년 4635억 원, 지난해 6053억 원 까지 성장했다. 하지만, 2022년과 2023년 각각 영업 손실 약 1310억 원, 1570억 원을 기록하며 적자 폭은 깊어지고 있다. 이를 개선하기 위해 위메이드는 수익성이 적은 사업 정리에 나서고 있다. 실제, 박 대표는 지난 3월 개최된 정기주주총회에서 \"적자가 커 회사 비용을 최적화 해야 한다\"고 언급한 바 있다. 위메이드는 미르M 국내외 서비스를 종료하겠다고 밝혔으며 영업 비용이 많이 발생했던 블록체인 사업 교통정리에도 나서고 있다. 박 대표가 취임한 이후 종료 및 축소 공지를 내린 블록체인 서비스만 10여개나 된다.업계는 위메이드가 게임 산업에 집중하기 위해 이런 전략을 구성했다고 평가했다. 업계 관계자는 \"세계적으로 블록체인·NFT(대체불가토큰) 관련 사업에 대한 기대감이 떨어지고 있다\"며 \":이런 상황에서 위메이드가 선택과 집중을 가져가는 것은 자연스러운 현상\"이라고 말했다. 다행히 위메이드의 게임 사업 분위기는 좋은 상황이다. 지난 1분기 출시된 '나이트크로우' 글로벌 버전이 론칭 3일 만에 누적 매출 1000만 달러(이날 기준138억1700만 원)을 기록하는 등 호성적을 기록했다. 또 하반기 기대작 '레전드 오브 이미르'가 출시될 예정이다. 미르4와 미르M 중국 출시도 위메이드 성적 개선에 키포인트다. 위메이드가 2001년 부터 중국에서 '미르의 전설2'를 서비스했던 역량을 바탕으로 중국 시장 선점에 나설 것으로 예상된다. 2020년 보스턴컨설팅그룹(BCG)에 따르면 중국 내 미르 IP의 시장 규모는 대해 연간 9조 원으로 평가받는다. 증권가는 위메이드가 이런 전략에 힘입어 3분기부터 실적 회복에 나설 것이라고 내다봤다. NH 투자증권에 따르면 위메이드는 3분기부터 영업이익 99억7000억 원을 기록하며 흑자전환할 것으로 예상된다.  한편, 위메이드는 블록체인 게임 플랫폼 '위믹스 플레이(WEMIX PLAY)의 개편을 진행한다. 이를 위해 오는 16일 '위믹스 데이'를 개최하고 관련 내용을 공유한다. 위믹스 데이에서는 새롭게 선보일 고도화된 블록체인 서비스와 토크노믹스가 처음으로 공개된다. 또 블록체인 게임 이용자, 커뮤니티 등 위믹스 생태계 전반이 함께 성장할 수 있도록, 토크노믹스를 강화하고 고품질 게임을 온보딩해 커뮤니티 중심 플랫폼으로 변화할 예정이다. 위메이드 관계자는 \"위믹스 플레이와 위믹스 리퍼블릭 중심으로 위믹스 생태계를 키워나갈 예정\"이라며 \"현재 서비스 중인 게임들과 앞으로 다가오는 신작 흥행에도 집중해 호성적을 기록할 것\"이라고 말했다.  \\</article>|\n",
    "\n",
    "<br>\n",
    "\n",
    "|역할|프롬프트|\n",
    "|---|---|\n",
    "|SYSTEM|논문 초록과 제안된 논문 제목이 제공됩니다. 논문 제목은 독자가 논문의 주제를 잘 파악할 수 있으면서도 눈길을 끌 수 있어야 합니다. 제목이 이러한 기준을 충족하지 않는 경우 5가지 대안을 제안하세요.|\n",
    "|USER| 초록: 최근 생성형 AI 기술의 발전으로 인해 대형 언어 모델(Large Language Model, LLM)의 활용 및 도입이 확대되고 있는 상황에서 기존 연구들은 기업내부 데이터의 활용에 대한 실제 적용사례나 구현방법을 찾아보기 힘들다. 이에 따라 본 연구에서는 가장 많이 이용되고 있는 LangChain 프레임워크를 이용한 LLM 애플리케이션 아키텍처를 활용하여 생성형 AI 서비스를 구현하는 방법을 제시한다. 이를 위해 LLM의 활용을 중심으로, 정보 부족 문제를 극복하는 다양한 방법을 검토하고 구체적인 해결책을 제시하였다. 이를 위해 파인튜닝이나 직접 문서 정보를 활용하는 방법을 분석하며, 이러한 문제를 해결하기 위한 RAG 모델을 활용한 정보 저장 및 검색 방법에 대해 주요단계에 대해 자세하게 살펴본다. 특히, RAG 모델을 활용하여 정보를 벡터저장소에 저장하고 검색하기 위한 방법으로 유사문맥 추천 및 QA시스템을 활용하였다. 또한 구체적인 작동 방식과 주요한 구현 단계 및 사례를 구현소스 및 사용자 인터페이스까지 제시하여 생성형 AI 기술에 대한 이해를 높였다. 이를 통해 LLM을 활용한 기업내 서비스 구현에 적극적으로 활용할 수 있도록 하는데 의미와 가치가 있다.<br> 제목: LLM 애플리케이션 아키텍처를 활용한 생성형 AI 서비스 구현: RAG모델과 LangChain 프레임워크 기반 |\n",
    "\n",
    "이와 같이 간단한 작업의 경우 구분 기호를 사용해도 출력 품질에 차이가 없을 수 있습니다. 하지만 작업이 복잡할수록 작업 세부 사항을 명확히 구분하는 것이 더 중요합니다. 모델에게 무엇을 요구하는지 정확히 이해하기 위해 모델이 고민하게 하지 마세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "성장의 가치 중시하고  \n",
      "책임 성취 더불어  \n",
      "꿈 현실로 만드는 곳, 위메이드의 비전\n"
     ]
    }
   ],
   "source": [
    "user_msg = HumanMessage(content='3연속 큰따옴표로 구분된 텍스트를 시조로 요약하세요.\"\"\"위메이드는 성장의 가치, 책임의 가치 그리고 성취의 가치를 중요시합니다. 누군가의 꿈이 현실로 이루어지면서 세상은 더 좋은 곳으로 발전하고 있습니다. 게임에서 시작하여 다양한 문화의 영역에서 무한한 상상을 현실로 만드는 창조집단, 그것이 위메이드의 비전입니다.\"\"\"')\n",
    "response = llm.invoke([user_msg])\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제공된 제목은 논문의 내용을 잘 반영하고 있지만, 더 눈길을 끌고 주제를 명확히 전달할 수 있는 대안을 제시하겠습니다.\n",
      "\n",
      "1. **기업 내 데이터 활용을 위한 생성형 AI 서비스 구현: LangChain과 RAG 모델의 통합**\n",
      "2. **LangChain과 RAG 모델을 이용한 LLM 기반 생성형 AI 서비스 아키텍처**\n",
      "3. **대형 언어 모델을 활용한 기업용 생성형 AI 서비스: LangChain과 RAG 모델의 적용**\n",
      "4. **LLM과 LangChain을 이용한 생성형 AI 서비스 구현 방법론: RAG 모델 중심으로**\n",
      "5. **기업 데이터를 활용한 생성형 AI 서비스: LangChain과 RAG 모델의 실용적 접근**\n"
     ]
    }
   ],
   "source": [
    "system_msg= SystemMessage(content=\"\"\"\n",
    "논문 초록과 제안된 논문 제목이 제공됩니다. 논문 제목은 독자가 논문의 주제를 잘 파악할 수 있으면서도 눈길을 끌 수 있어야 합니다. 제목이 이러한 기준을 충족하지 않는 경우 5가지 대안을 제안하세요.\n",
    "\"\"\")\n",
    "user_msg = HumanMessage(content=\"\"\"\n",
    "초록: 최근 생성형 AI 기술의 발전으로 인해 대형 언어 모델(Large Language Model, LLM)의 활용 및 도입이 확대되고 있는 상황에서 기존 연구들은 기업내부 데이터의 활용에 대한 실제 적용사례나 구현방법을 찾아보기 힘들다. 이에 따라 본 연구에서는 가장 많이 이용되고 있는 LangChain 프레임워크를 이용한 LLM 애플리케이션 아키텍처를 활용하여 생성형 AI 서비스를 구현하는 방법을 제시한다. 이를 위해 LLM의 활용을 중심으로, 정보 부족 문제를 극복하는 다양한 방법을 검토하고 구체적인 해결책을 제시하였다. 이를 위해 파인튜닝이나 직접 문서 정보를 활용하는 방법을 분석하며, 이러한 문제를 해결하기 위한 RAG 모델을 활용한 정보 저장 및 검색 방법에 대해 주요단계에 대해 자세하게 살펴본다. 특히, RAG 모델을 활용하여 정보를 벡터저장소에 저장하고 검색하기 위한 방법으로 유사문맥 추천 및 QA시스템을 활용하였다. 또한 구체적인 작동 방식과 주요한 구현 단계 및 사례를 구현소스 및 사용자 인터페이스까지 제시하여 생성형 AI 기술에 대한 이해를 높였다. 이를 통해 LLM을 활용한 기업내 서비스 구현에 적극적으로 활용할 수 있도록 하는데 의미와 가치가 있다.\n",
    "제목: LLM 애플리케이션 아키텍처를 활용한 생성형 AI 서비스 구현: RAG모델과 LangChain 프레임워크 기반\n",
    "\"\"\")\n",
    "\n",
    "response = llm.invoke([system_msg,user_msg])\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 방안 4: 작업 완료에 필요한 단계 지정하기\n",
    "\n",
    "일부 복잡한 작업들은 일련의 단계로 나누어 지정하는 것이 좋습니다. 단계를 명시적으로 작성하면 모델이 더 쉽게 따라갈 수 있습니다.\n",
    "\n",
    "|역할|프롬프트|\n",
    "|---|---|\n",
    "|SYSTEM|다음 단계별 지침에 따라 사용자 입력에 응답합니다.<br>1단계 - 사용자가 3연속 큰따옴표로 묶은 텍스트를 제공합니다. 이 텍스트를 \"요약:\"이라는 접두사를 사용하여 한 문장으로 요약합니다.<br>2단계 - 1단계의 요약을 영어로 번역하고 \"번역:\"이라는 접두어를 사용하여 제공합니다..|\n",
    "|USER|\"\"\"위메이드의 핵심 가치는 성장, 책임, 성취입니다.위메이드의 목적은 개인의 책임 완수를 통한 성취를 통해 기업이 성장하는데 있으며, 이를 법규나 규율로 강제하지 않고 비전의 공유를 통한 목표의 일치, 역할과 그에 걸맞는 권한의 부여 그리고 공정하고 명확한 보상을 통해 선순환 할 수 있는 가치 체계를 공감하고 체득할 수 있게 하는데, 이를 위메이드 e라 부릅니다. 위메이드의 가치 체계는 소문자 e의 궤적을 그리며 순환합니다. e가 상징하는 바는 다음과 같습니다. evolution(진화) 회사의 성장은 일회성 이벤트가 아니며, 반복적이고 지속적이며 이를 통해 회사가 양적 성장 뿐만 아니라 질적 성장과 함께 시장과 시대의 변화에 맞추어 진화하는 것을 그 목표로 하고 있습니다. eco system(생태계) 위메이드의 가치는 비상식적이거나 도달 불가능한 구호가 아닌, 개인이 자신의 위치에서 적합한 역할을 수행할 때 자연스러운 전체가 되어 목표를 이룰 수 있는 가치 생태계의 구축을 그 목표로 하고 있습니다. exponential(기하급수적) 위메이드의 가치 체계는 성장이 반복될 때마다 회사의 성장 속도와 폭은 점점 빨라지고 커지는 기하급수적 성장을 가능하게 하는 선순환 구조를 이루고 있습니다.\"\"\"|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요약: 위메이드는 성장, 책임, 성취를 핵심 가치로 하며, 개인의 책임 완수를 통해 기업이 성장하는 선순환 구조를 목표로 합니다.\n",
      "\n",
      "번역: Wemade's core values are growth, responsibility, and achievement, aiming for a virtuous cycle where the company grows through the fulfillment of individual responsibilities.\n"
     ]
    }
   ],
   "source": [
    "system_msg= SystemMessage(content=\"\"\"\n",
    "다음 단계별 지침에 따라 사용자 입력에 응답합니다.\n",
    "1단계 - 사용자가 3연속 큰따옴표로 묶은 텍스트를 제공합니다. 이 텍스트를 \"요약:\"이라는 접두사를 사용하여 한 문장으로 요약합니다.\n",
    "2단계 - 1단계의 요약을 영어로 번역하고 \"번역:\"이라는 접두어를 사용하여 제공합니다.\n",
    "\"\"\")\n",
    "user_msg = HumanMessage(content='\"\"\"위메이드의 핵심 가치는 성장, 책임, 성취입니다.위메이드의 목적은 개인의 책임 완수를 통한 성취를 통해 기업이 성장하는데 있으며, 이를 법규나 규율로 강제하지 않고 비전의 공유를 통한 목표의 일치, 역할과 그에 걸맞는 권한의 부여 그리고 공정하고 명확한 보상을 통해 선순환 할 수 있는 가치 체계를 공감하고 체득할 수 있게 하는데, 이를 위메이드 e라 부릅니다. 위메이드의 가치 체계는 소문자 e의 궤적을 그리며 순환합니다. e가 상징하는 바는 다음과 같습니다. evolution(진화) 회사의 성장은 일회성 이벤트가 아니며, 반복적이고 지속적이며 이를 통해 회사가 양적 성장 뿐만 아니라 질적 성장과 함께 시장과 시대의 변화에 맞추어 진화하는 것을 그 목표로 하고 있습니다. eco system(생태계) 위메이드의 가치는 비상식적이거나 도달 불가능한 구호가 아닌, 개인이 자신의 위치에서 적합한 역할을 수행할 때 자연스러운 전체가 되어 목표를 이룰 수 있는 가치 생태계의 구축을 그 목표로 하고 있습니다. exponential(기하급수적) 위메이드의 가치 체계는 성장이 반복될 때마다 회사의 성장 속도와 폭은 점점 빨라지고 커지는 기하급수적 성장을 가능하게 하는 선순환 구조를 이루고 있습니다.\"\"\"')\n",
    "\n",
    "\n",
    "response = llm.invoke([system_msg,user_msg])\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 방안 5 : 예제 제공 \n",
    "모든 예제에 적용되는 일반적인 지침을 제공하는 것이 일반적으로 작업의 모든 변형을 예제로 보여주는 것보다 더 효율적이지만 경우에 따라서는 예제를 제공하는 것이 더 쉬울 수도 있습니다. 예를 들어 모델이 명시적으로 설명하기 어려운 특정 스타일의 사용자 쿼리 응답을 모방하도록 하려는 경우입니다. 이를 \"few-shot\" 프롬프트라고 합니다. 제공하는 예제의 수에 따라 zero-shot , one-shot, few-shot 프롬프팅으로 분류합니다. \n",
    "\n",
    "|역할|프롬프트|\n",
    "|---|---|\n",
    "|SYSTEM|사용자의 질의에 일관된 스타일로 답변하세요. 예를 들어, 사용자가 인내심에 대해 가르쳐달라고 요청하면, \"\"\"가장 깊은 계곡을 깎아내는 강은 소박한 샘에서 기원하고, 가장 웅장한 교향곡은 하나의 음표에서 시작되며, 가장 복잡한 태피스트리는 한가닥 실에서 시작됩니다.\"\"\"와 같이 답합니다. |\n",
    "|USER|바다에 대해 가르쳐주세요|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바다는 대지의 심장과 같아, 그 맥박은 조수의 흐름을 통해 느낄 수 있습니다. 가장 깊은 심연 속에는 미지의 생명들이 숨쉬고, 가장 잔잔한 수면 위에는 바람의 속삭임이 머무릅니다. 그 끝없는 수평선은 우리의 상상력을 자극하며, 바다의 무한한 깊이는 우리의 영혼을 탐구하게 만듭니다. 바다는 우리에게 겸손함을 가르치고, 그 위대한 힘과 아름다움은 우리의 일상 속에서 경외감을 불러일으킵니다.\n"
     ]
    }
   ],
   "source": [
    "system_msg= SystemMessage(content=\"\"\"\n",
    "사용자의 질의에 일관된 스타일로 답변하세요. 예를 들어, 사용자가 인내심에 대해 가르쳐달라고 요청하면, \"가장 깊은 계곡을 깎아내는 강은 소박한 샘에서 기원하고, 가장 웅장한 교향곡은 하나의 음표에서 시작되며, 가장 복잡한 태피스트리는 한가닥 실에서 시작됩니다.\"와 같이 답합니다.\n",
    "\"\"\")\n",
    "user_msg = HumanMessage(content='바다에 대해 가르쳐주세요')\n",
    "\n",
    "\n",
    "response = llm.invoke([system_msg,user_msg])\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 방안 6: 원하는 출력 길이 지정\n",
    "모델에 지정된 목표 길이의 출력을 생성하도록 요청할 수 있습니다. 목표 출력 길이는 단어, 문장, 단락, 글머리 기호 등의 개수로 지정할 수 있습니다. 그러나 모델에 특정 단어 수를 생성하도록 지시하는 것은 높은 정확도로 작동하지 않는다는 점에 유의하세요. 모델은 특정 수의 단락 또는 글머리 기호로 출력을 더 안정적으로 생성할 수 있습니다.\n",
    "\n",
    "|역할|프롬프트|\n",
    "|---|---|\n",
    "|USER|3연속 큰따옴표로 구분된 텍스트를 50단어 내로 요약해주세요. \"\"\"텍스트 입력\"\"\"|\n",
    "|USER|3연속 큰따옴표로 구분된 텍스트를 2 문단 내로 요약해주세요. \"\"\"텍스트 입력\"\"\"|\n",
    "|USER|3연속 큰따옴표로 구분된 텍스트를 50단어 내로 요약해주세요. \"\"\"텍스트 입력\"\"\"|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전략 2 : 참조 텍스트 제공\n",
    "\n",
    "#### 방안 1 : 모델에게 참조 텍스트를 사용하여 응답하도록 지시하기\n",
    "현재 쿼리와 관련된 신뢰할 수 있는 정보를 모델에 제공할 수 있다면 모델에 제공된 정보를 사용하여 답변을 작성하도록 지시할 수 있습니다. \n",
    "\n",
    "|역할|프롬프트|\n",
    "|---|----|\n",
    "|SYSTEM|제공된 도움말을 큰따옴표로 구분하여 질문에 답하세요. 문서에서 답을 찾을 수 없는 경우에는 \"답을 찾을 수 없습니다.\"라고 작성합니다.|\n",
    "|USER|\\<insert articles, each delimited by triple quotes\\><br>Question: \\<insert question here\\>|\n",
    "\n",
    "모든 모델의 컨텍스트 창이 제한되어 있기 때문에 질문과 관련된 정보를 동적으로 조회할 수 있는 방법이 필요합니다. 보통 이를 위하여 Keyword 검색, Vector database와 Embedding model을 활용한 semantic search, knowledge graph를 활용한 지식 탐색 등의 기술을 활용하여 검색 증강 생성 : RAG (Retrieval Augmented Generation)을 구현합니다. RAG 자체는 오늘 실습 범위를 넘어가므로 이러한 개념이 있다는 것을 우선 이해하도록 합니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 방안 2: 모델에게 참조 텍스트를 인용하여 답변하도록 지시하기\n",
    "위에서 기술된 RAG에 의해서 사용자 입력에 관련 지식이 보완된 경우, 제공된 문서의 구절을 참조하여 모델에 답변에 인용문을 추가하도록 요청하는 것은 간단합니다. 그런 다음 제공된 문서 내에서 문자열 일치를 통해 출력의 인용을 프로그래밍 방식으로 확인할 수 있습니다.\n",
    "\n",
    "|역할|프롬프트|\n",
    "|----|----|\n",
    "|SYSTEM|3중 따옴표로 구분된 문서와 질문이 제공됩니다. 제공된 문서만을 사용하여 질문에 답하고 질문에 답하는 데 사용된 문서의 구절을 인용해야 합니다. 문서에 이 질문에 답하는 데 필요한 정보가 포함되어 있지 않은 경우 그냥 \"정보 불충분\"이라고 응답하십시오. 질문에 대한 답변이 제공된 경우 반드시 인용 주석을 달아야 합니다. 관련 구절을 인용하려면 다음 형식을 사용합니다({\"인용\": ...}).|\n",
    "|USER|\"\"\"'문서삽입'\"\"\"<br>질문:'질문삽입'|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전략 3: 복잡한 작업을 더 간단한 하위 작업으로 나누기 \n",
    "#### 방안 1 : 의도 분류를 사용하여 사용자 쿼리에 가장 관련성이 높은 명령어를 식별하기 \n",
    "다양한 경우를 처리하기 위해 많은 독립적인 명령어 집합이 필요한 작업의 경우 먼저 쿼리 유형을 분류하고 그 분류를 사용하여 어떤 명령어가 필요한지 결정하는 것이 유용할 수 있습니다. 이는 고정 카테고리를 정의하고 해당 카테고리의 작업을 처리하는 데 관련된 지침을 정의함으로써 달성할 수 있습니다. 이 프로세스를 재귀적으로 적용하여 작업을 일련의 단계로 분해할 수도 있습니다. 이 접근 방식의 장점은 각 쿼리에는 작업의 다음 단계를 수행하는 데 필요한 명령어만 포함되므로 전체 작업을 수행하는 데 단일 쿼리를 사용하는 것보다 오류율이 낮아질 수 있다는 것입니다. 또한 프롬프트가 클수록 실행 비용이 더 많이 들기 때문에 비용도 절감할 수 있습니다. 예를 들어 고객 서비스 애플리케이션의 경우 쿼리를 다음과 같이 유용하게 분류할 수 있다고 가정해 보겠습니다:\n",
    "\n",
    "|역할|프롬프트|\n",
    "|---|----|\n",
    "|SYSTEM|고객 지원 서비스를 제공해야 합니다. 각 고객의 질의를 기본 카테고리와 보조 카테고리로 분류합니다. 기본 및 보조 키가 포함된 json 형식의 출력을 제공합니다. 기본 카테고리: 청구, 기술 지원, 계정 관리 또는 일반 문의<br>청구 보조 카테고리: <br>- 구독 취소 또는 업그레이드 <br>- 결제 방법 추가 <br>- 청구 설명 <br>- 청구 이의 제기 <br>기술 지원 보조 카테고리: <br>- 문제 해결 <br>- 장치 호환성 <br>- 소프트웨어 업데이트 <br>계정 관리 보조 카테고리: <br>- 비밀번호 재설정 <br>- 개인 정보 업데이트 <br>- 계정 폐쇄 <br>- 계정 보안 <br>일반 문의 보조 카테고리: <br>- 제품 정보 <br>- 가격 <br>- 피드백 <br>- 상담원에게 문의하기|\n",
    "|USER|인터넷이 동작하지 않습니다. 인터넷 수리가 필요합니다.|\n",
    "\n",
    "고객 문의의 분류에 따라 모델이 다음 단계를 처리할 수 있도록 보다 구체적인 지침을 모델에 제공할 수 있습니다. 예를 들어 고객이 '문제 해결'에 대한 도움이 필요하다고 가정해 보겠습니다.\n",
    "\n",
    "|역할|프롬프트|\n",
    "|---|----|\n",
    "|SYSTEM|기술 지원 맥락에서 문제 해결이 필요한 고객 서비스 문의를 받게 됩니다. 다음과 같은 방법으로 사용자를 도와주세요. <br>- 라우터에 연결된 모든 케이블이 연결되어 있는지 확인하도록 요청하세요. 시간이 지나면 케이블이 느슨해지는 것이 일반적이라는 점에 유의하세요. <br>- 모든 케이블이 연결되었는데도 문제가 지속되면 사용 중인 라우터 모델을 물어보세요. <br>- 이제 장치를 다시 시작하는 방법을 알려주세요: <br>-- 모델 번호가 MTD-327J인 경우 빨간색 버튼을 5초간 누른 다음 5분간 기다렸다가 연결을 테스트하라고 알려주세요. <br>-- 모델 번호가 MTD-327S인 경우 플러그를 뽑았다가 다시 연결하고 5분간 기다렸다가 연결을 테스트하라고 알려주세요.<br>- 기기를 다시 시작하고 5분을 기다린 후에도 고객의 문제가 지속되면 {\"IT 지원 요청됨\"}을 출력하여 IT 지원팀에 연결합니다. <br>- 사용자가 이 주제와 관련 없는 질문을 시작하면 문제 해결에 대한 현재 채팅을 종료할 것인지 확인하고 다음 체계에 따라 요청을 분류합니다: <위에서 기본/보조 분류 체계 삽입>|\n",
    "|USER|인터넷이 동작하지 않습니다. 인터넷 수리가 필요합니다.|\n",
    "\n",
    "대화의 상태가 변경될 때를 나타내는 특수 문자열을 방출하도록 모델에 지시를 내렸음을 알 수 있습니다. 이를 통해 시스템을 상태에 따라 주입되는 명령어가 결정되는 상태 머신으로 전환할 수 있습니다. 상태, 해당 상태와 관련된 명령어, 그리고 선택적으로 해당 상태에서 허용되는 상태 전환을 추적함으로써 덜 구조화된 접근 방식으로는 달성하기 어려운 사용자 경험에 가드레일을 설치할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"기본 카테고리\": \"기술 지원\",\n",
      "  \"보조 카테고리\": \"문제 해결\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "system_msg= SystemMessage(content=\"\"\"\n",
    "고객 지원 서비스를 제공해야 합니다. 각 고객의 질의를 기본 카테고리와 보조 카테고리로 분류합니다. 기본 및 보조 키가 포함된 json 형식의 출력을 제공합니다. 기본 카테고리: 청구, 기술 지원, 계정 관리 또는 일반 문의\n",
    "청구 보조 카테고리: \n",
    "- 구독 취소 또는 업그레이드\n",
    "- 결제 방법 추가\n",
    "- 청구 설명 \n",
    "- 청구 이의 제기 \n",
    "기술 지원 보조 카테고리: \n",
    "- 문제 해결 \n",
    "- 장치 호환성 \n",
    "- 소프트웨어 업데이트 \n",
    "계정 관리 보조 카테고리: \n",
    "- 비밀번호 재설정\n",
    "- 개인 정보 업데이트\n",
    "- 계정 폐쇄\n",
    "- 계정 보안 \n",
    "일반 문의 보조 카테고리: \n",
    "- 제품 정보 \n",
    "- 가격 \n",
    "- 피드백 \n",
    "- 상담원에게 문의하기\n",
    "\"\"\")\n",
    "user_msg = HumanMessage(content='인터넷이 동작하지 않습니다. 인터넷 수리가 필요합니다.')\n",
    "\n",
    "response = llm.invoke([system_msg,user_msg])\n",
    "print(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "먼저 라우터에 연결된 모든 케이블이 제대로 연결되어 있는지 확인해 주시겠어요? 시간이 지나면 케이블이 느슨해질 수 있습니다. 모든 케이블이 제대로 연결되어 있는지 확인해 주세요.\n"
     ]
    }
   ],
   "source": [
    "system_msg = SystemMessage(content=\"\"\"\n",
    "기술 지원 맥락에서 문제 해결이 필요한 고객 서비스 문의를 받게 됩니다. 다음과 같은 방법으로 사용자를 도와주세요. \n",
    "- 라우터에 연결된 모든 케이블이 연결되어 있는지 확인하도록 요청하세요. 시간이 지나면 케이블이 느슨해지는 것이 일반적이라는 점에 유의하세요. \n",
    "- 모든 케이블이 연결되었는데도 문제가 지속되면 사용 중인 라우터 모델을 물어보세요. \n",
    "- 이제 장치를 다시 시작하는 방법을 알려주세요: \n",
    "-- 모델 번호가 MTD-327J인 경우 빨간색 버튼을 5초간 누른 다음 5분간 기다렸다가 연결을 테스트하라고 알려주세요. \n",
    "-- 모델 번호가 MTD-327S인 경우 플러그를 뽑았다가 다시 연결하고 5분간 기다렸다가 연결을 테스트하라고 알려주세요.\n",
    "- 기기를 다시 시작하고 5분을 기다린 후에도 고객의 문제가 지속되면 {\"IT 지원 요청됨\"}을 출력하여 IT 지원팀에 연결합니다. \n",
    "- 사용자가 이 주제와 관련 없는 질문을 시작하면 문제 해결에 대한 현재 채팅을 종료할 것인지 확인합니다.\n",
    "\"\"\")\n",
    "user_msg = HumanMessage(content='인터넷이 동작하지 않습니다. 인터넷 수리가 필요합니다. 어떻게 해야 하나요?')\n",
    "\n",
    "response = llm.invoke([system_msg,user_msg])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 방안 2 : 매우 긴 대화가 필요한 대화 애플리케이션의 경우 이전 대화를 요약하거나 필터링하기 \n",
    "모델에는 고정된 컨텍스트 길이가 있으므로 전체 대화가 컨텍스트 창에 포함되는 사용자와 어시스턴트 간의 대화를 무한정 계속할 수 없습니다. 이 문제에 대한 다양한 해결 방법이 있는데, 그 중 하나는 대화의 이전 회차를 요약하는 것입니다. 입력의 크기가 미리 정해진 임계값 길이에 도달하면 대화의 일부를 요약하는 쿼리가 트리거되고 이전 대화의 요약이 시스템 메시지의 일부로 포함될 수 있습니다. 또는 전체 대화에 걸쳐 백그라운드에서 비동기적으로 이전 대화가 요약될 수도 있습니다. 또 다른 해결책은 현재 쿼리와 가장 관련성이 높은 대화의 이전 부분을 동적으로 선택하는 것입니다. 이전 대화들을 Archiving하고 위에서 언급했던 검색증강 생성을 통해 컨텍스트 길이 제한을 극복할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 방안 3: 긴 문서를 조각별로 요약하고 전체 요약을 재귀적으로 구성하기 \n",
    "모델에는 고정된 문맥 길이가 있으므로 단일 쿼리에서 생성된 요약의 길이를 뺀 문맥 길이보다 긴 텍스트는 요약하는 데 사용할 수 없습니다. 책과 같이 매우 긴 문서를 요약하려면 일련의 쿼리를 사용하여 문서의 각 섹션을 요약할 수 있습니다. 섹션 요약을 연결하고 요약하여 요약의 요약을 생성할 수 있습니다. 이 프로세스는 전체 문서가 요약될 때까지 재귀적으로 진행될 수 있습니다. 이후 섹션을 이해하기 위해 이전 섹션에 대한 정보를 사용해야 하는 경우, 책의 특정 지점 앞의 텍스트에 대한 실행 요약을 포함하면서 해당 지점의 내용을 요약하는 것이 유용할 수 있습니다. 책을 요약하는 이 절차의 효과는 OpenAI의 이전 연구에서 GPT-3의 변형을 사용하여 연구한 바 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
